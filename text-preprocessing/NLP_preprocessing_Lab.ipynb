{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><b><font size=\"6\">NLP_preprocessing_Lab1</font></b></strong>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#add your Name\n",
    "<strong><b><font size=\"4\">Name : TAHA NOUALI<font></b></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>To do tasks <span class=\"tocSkip\"></span></h1>\n",
    "    <li> Lower casing\n",
    "    <li> Punctuation removal\n",
    "    <li> Identifying and Remove Stop Words\n",
    "    <li> Tokenize Text in Words\n",
    "    <li> NLTK Word Stemming\n",
    "    <li> Build a bag-of-words\n",
    "    <li> Create Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    <li> Bag of words\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import tokenize\n",
    "\n",
    "#sci-kit learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Inspect the IMDB Movie Reviews Dataset\n",
    "- Labeled movie reviews data as either positive or negative reviews\n",
    "- We can download the __movie_reviews__ package using the nltk.download function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\tNouali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_reviews.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos/cv995_21821.txt',\n",
       " 'pos/cv996_11592.txt',\n",
       " 'pos/cv997_5046.txt',\n",
       " 'pos/cv998_14111.txt',\n",
       " 'pos/cv999_13106.txt']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __fileids__ can also filter the available files based on their category, which is the name of the subfolders they are located in. Therefore we can have lists of positive and negative reviews separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_fileids = movie_reviews.fileids('neg')\n",
    "positive_fileids = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_fileids), len(positive_fileids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can inspect one of the reviews using the raw method of movie_reviews, each file is split into sentences, the curators of this dataset also removed from each review from any direct mention of the rating of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by phil curtolo mel gibson ( braveheart ) gave a gripping performance as the father of a young kidnapped boy in ron howard's ransom . \n",
      "gibson plays tom mullen , a wealthy business tycoon whose past actions are coming back to haunt him as a deranged psychopath , played by gary sinise ( forrest gump ) , and his band of low-life thugs kidnap his only son for $2 million . \n",
      "tom and his wife , kate , played by rene russo ( tin cup ) were instructed not to inform the police , but they contacted the fbi . \n",
      "minutes later , an elite team of agents led by delroy lindo ( broken arrow ) are in tom's house and wiring every phone . \n",
      "the plot sounds average , just like most other kidnap movies that you've already seen , and it was nothing more than that . \n",
      "that is until about half-way through the movie . \n",
      "suddenly , tom goes to the fox 5 news room and makes a live broadcast saying , \" this is your ransom . \n",
      "but this is as close as you will ever get to it . \n",
      "instead , i am offering this money as a reward on your head , dead or alive . \" \n",
      "at this point , the plot thickened , and the unusually slow start of the film turned into a suspense-filled action film with great stunts . \n",
      "the last half of the film is very well done . \n",
      "another thing that carries this film are the superb performances by gibson and sinise , as they collide in a game of wits over their cellular phones for most of the movie . \n",
      "owen gleiberman of entertainment weekly commented on the subject : \" it makes you wonder what kidnappers did before cell phones . \" \n",
      "before this movie , sinise played mostly \" good guys , \" first in of mice & men , then in forrest gump , and most recently , in apollo 13 . \n",
      "but he was surprisingly devilish and cold in his portrayal of a cop-gone-bad . \n",
      "gibson , of course , was just being gibson , in an oscar-worthy performance . \n",
      "although most of the scenes were quite predictable , ransom is a very entertaining and suspenseful film . \n",
      " , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.raw(fileids=positive_fileids[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg/cv002_17424.txt'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_fileids[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_features = pd.DataFrame(\n",
    "    {'review':movie_reviews.raw(fileids=[f]),'label': 'neg'} for f in negative_fileids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_features = pd.DataFrame(\n",
    "    {'review':movie_reviews.raw(fileids=[f]),'label': 'pos'} for f in positive_fileids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([positive_features, negative_features], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you've got mail works alot better than it dese...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" jaws \" is a rare film that grabs your atten...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>if anything , \" stigmata \" should be taken as ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>john boorman's \" zardoz \" is a goofy cinematic...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>the kids in the hall are an acquired taste . \\...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>two party guys bob their heads to haddaway's d...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review label\n",
       "0     films adapted from comic books have had plenty...   pos\n",
       "1     every now and then a movie comes along from a ...   pos\n",
       "2     you've got mail works alot better than it dese...   pos\n",
       "3      \" jaws \" is a rare film that grabs your atten...   pos\n",
       "4     moviemaking is a lot like being the general ma...   pos\n",
       "...                                                 ...   ...\n",
       "1995  if anything , \" stigmata \" should be taken as ...   neg\n",
       "1996  john boorman's \" zardoz \" is a goofy cinematic...   neg\n",
       "1997  the kids in the hall are an acquired taste . \\...   neg\n",
       "1998  there was a time when john carpenter was a gre...   neg\n",
       "1999  two party guys bob their heads to haddaway's d...   neg\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"]=data[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you've got mail works alot better than it deserves to . \\nin order to make the film a success , all they had to do was cast two extremely popular and attractive stars , have them share the screen for about two hours and then collect the profits . \\nno real acting was involved and there is not an original or inventive bone in it's body ( it's basically a complete re-shoot of the shop around the corner , only adding a few modern twists ) . \\nessentially , it goes against and defies all concepts of good contemporary filmmaking . \\nit's overly sentimental and at times terribly mushy , not to mention very manipulative . \\nbut oh , how enjoyable that manipulation is . \\nbut there must be something other than the casting and manipulation that makes the movie work as well as it does , because i absolutely hated the previous ryan/hanks teaming , sleepless in seattle . \\nit couldn't have been the directing , because both films were helmed by the same woman . \\ni haven't quite yet figured out what i liked so much about you've got mail , but then again , is that really important ? \\nif you like something so much , why even question it ? \\nagain , the storyline is as cliched as they come . \\ntom hanks plays joe fox , the insanely likeable owner of a discount book chain and meg ryan plays kathleen kelley , the even more insanely likeable proprietor of a family-run children's book shop called , in a nice homage , the shop around the corner . \\nfox and kelley soon become bitter rivals because the new fox books store is opening up right across the block from the small business . \\nlittle do they know , they are already in love with each other over the internet , only neither party knows the other person's true identity . \\nthe rest of the story isn't important because all it does is serve as a mere backdrop for the two stars to share the screen . \\nsure , there are some mildly interesting subplots , but they all fail in comparison to the utter cuteness of the main relationship . \\nall of this , of course , leads up to the predictable climax . \\nbut as foreseeable as the ending is , it's so damn cute and well-done that i doubt any movie in the entire year contains a scene the evokes as much pure joy as this part does . \\nwhen ryan discovers the true identity of her online love , i was filled with such , for lack of a better word , happiness that for the first time all year , i actually left the theater smiling . \\n\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.review[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tahahdjhzuahuzh'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st=\"taha;hdjhzuahuzh.()\"\n",
    "\"\".join(list(filter(lambda x:x not in punctuations,st )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tahahdjhzuahuzh'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[%s]' % re.escape(string.punctuation), '' , st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing ponctuations\n",
    "data['review']=data['review'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'youve got mail works alot better than it deserves to  \\nin order to make the film a success  all they had to do was cast two extremely popular and attractive stars  have them share the screen for about two hours and then collect the profits  \\nno real acting was involved and there is not an original or inventive bone in its body  its basically a complete reshoot of the shop around the corner  only adding a few modern twists   \\nessentially  it goes against and defies all concepts of good contemporary filmmaking  \\nits overly sentimental and at times terribly mushy  not to mention very manipulative  \\nbut oh  how enjoyable that manipulation is  \\nbut there must be something other than the casting and manipulation that makes the movie work as well as it does  because i absolutely hated the previous ryanhanks teaming  sleepless in seattle  \\nit couldnt have been the directing  because both films were helmed by the same woman  \\ni havent quite yet figured out what i liked so much about youve got mail  but then again  is that really important  \\nif you like something so much  why even question it  \\nagain  the storyline is as cliched as they come  \\ntom hanks plays joe fox  the insanely likeable owner of a discount book chain and meg ryan plays kathleen kelley  the even more insanely likeable proprietor of a familyrun childrens book shop called  in a nice homage  the shop around the corner  \\nfox and kelley soon become bitter rivals because the new fox books store is opening up right across the block from the small business  \\nlittle do they know  they are already in love with each other over the internet  only neither party knows the other persons true identity  \\nthe rest of the story isnt important because all it does is serve as a mere backdrop for the two stars to share the screen  \\nsure  there are some mildly interesting subplots  but they all fail in comparison to the utter cuteness of the main relationship  \\nall of this  of course  leads up to the predictable climax  \\nbut as foreseeable as the ending is  its so damn cute and welldone that i doubt any movie in the entire year contains a scene the evokes as much pure joy as this part does  \\nwhen ryan discovers the true identity of her online love  i was filled with such  for lack of a better word  happiness that for the first time all year  i actually left the theater smiling  \\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verification\n",
    "data.review[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and Remove Stop Words\n",
    "- A stop word is a commonly used word (such as “the”, “a”, “an”, “in”). \n",
    "- Removal of stopwords is necessary since they add noise without having any informational value in modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Download and check stopwords from nltk:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tNouali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stop words: 179\n"
     ]
    }
   ],
   "source": [
    "print('Total stop words:',len(stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Removing stopwords from reviews:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing stop words\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop])\n",
    "\n",
    "data[\"review\"]=data[\"review\"].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'youve got mail works alot better deserves order make film success cast two extremely popular attractive stars share screen two hours collect profits real acting involved original inventive bone body basically complete reshoot shop around corner adding modern twists essentially goes defies concepts good contemporary filmmaking overly sentimental times terribly mushy mention manipulative oh enjoyable manipulation must something casting manipulation makes movie work well absolutely hated previous ryanhanks teaming sleepless seattle couldnt directing films helmed woman havent quite yet figured liked much youve got mail really important like something much even question storyline cliched come tom hanks plays joe fox insanely likeable owner discount book chain meg ryan plays kathleen kelley even insanely likeable proprietor familyrun childrens book shop called nice homage shop around corner fox kelley soon become bitter rivals new fox books store opening right across block small business little know already love internet neither party knows persons true identity rest story isnt important serve mere backdrop two stars share screen sure mildly interesting subplots fail comparison utter cuteness main relationship course leads predictable climax foreseeable ending damn cute welldone doubt movie entire year contains scene evokes much pure joy part ryan discovers true identity online love filled lack better word happiness first time year actually left theater smiling'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verification\n",
    "data.review[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text in Words\n",
    "- Tokenizing text is important since text can’t be processed without tokenization. Tokenization process means splitting bigger parts to small parts.\n",
    "- You can tokenize paragraphs to sentences and tokenize sentences to words according to your needs. \n",
    "- NLTK is shipped with sentence tokenizer and word tokenizer.\n",
    "- Non english text can also be tokenized by specifying the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tNouali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word Tokenizing:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review']=data['review'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[films, adapted, comic, books, plenty, success...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[every, movie, comes, along, suspect, studio, ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[youve, got, mail, works, alot, better, deserv...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[jaws, rare, film, grabs, attention, shows, si...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[moviemaking, lot, like, general, manager, nfl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>[anything, stigmata, taken, warning, releasing...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>[john, boormans, zardoz, goofy, cinematic, deb...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>[kids, hall, acquired, taste, took, least, sea...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>[time, john, carpenter, great, horror, directo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>[two, party, guys, bob, heads, haddaways, danc...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review label\n",
       "0     [films, adapted, comic, books, plenty, success...   pos\n",
       "1     [every, movie, comes, along, suspect, studio, ...   pos\n",
       "2     [youve, got, mail, works, alot, better, deserv...   pos\n",
       "3     [jaws, rare, film, grabs, attention, shows, si...   pos\n",
       "4     [moviemaking, lot, like, general, manager, nfl...   pos\n",
       "...                                                 ...   ...\n",
       "1995  [anything, stigmata, taken, warning, releasing...   neg\n",
       "1996  [john, boormans, zardoz, goofy, cinematic, deb...   neg\n",
       "1997  [kids, hall, acquired, taste, took, least, sea...   neg\n",
       "1998  [time, john, carpenter, great, horror, directo...   neg\n",
       "1999  [two, party, guys, bob, heads, haddaways, danc...   neg\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Word Stemming\n",
    "- Word stemming means removing affixes from words and return the root word. Ex: The stem of the word working => work.\n",
    "- Search engines use this technique when indexing pages, so many people write different versions for the same word and all of them are stemmed to the root word.\n",
    "- There are many algorithms for stemming, but the most used algorithm is Porter stemming algorithm.\n",
    "- NLTK has a class called PorterStemmer which uses Porter stemming algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [film, adapt, comic, book, plenti, success, wh...\n",
       "1       [everi, movi, come, along, suspect, studio, ev...\n",
       "2       [youv, got, mail, work, alot, better, deserv, ...\n",
       "3       [jaw, rare, film, grab, attent, show, singl, i...\n",
       "4       [moviemak, lot, like, gener, manag, nfl, team,...\n",
       "                              ...                        \n",
       "1995    [anyth, stigmata, taken, warn, releas, similar...\n",
       "1996    [john, boorman, zardoz, goofi, cinemat, debacl...\n",
       "1997    [kid, hall, acquir, tast, took, least, season,...\n",
       "1998    [time, john, carpent, great, horror, director,...\n",
       "1999    [two, parti, guy, bob, head, haddaway, danc, h...\n",
       "Name: review, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def steem_words(text):\n",
    "    return [stemmer.stem(word) for word in text]\n",
    "\n",
    "data[\"review\"].apply(lambda text: steem_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing Words Using [WordNet](https://wordnet.princeton.edu/)\n",
    "\n",
    "__Wordnet:__\n",
    "- WordNet is a lexical database for the English language.\n",
    "- It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. \n",
    "- WordNet can thus be seen as a combination of dictionary and thesaurus. While it is accessible to human users via a web browser, its primary use is in automatic text analysis and artificial intelligence applications.\n",
    "\n",
    "__Lemmatization:__\n",
    "- Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. \n",
    "- It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming.\n",
    "- Example: reduce words such as “am”, “are”, and “is” to a common form such as “be”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tNouali\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text])\n",
    "data[\"review\"] = data[\"review\"].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bag of Words using CountVectorizer:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0009f</th>\n",
       "      <th>000acre</th>\n",
       "      <th>000aweek</th>\n",
       "      <th>000foot</th>\n",
       "      <th>000paltry</th>\n",
       "      <th>007</th>\n",
       "      <th>007esque</th>\n",
       "      <th>00s</th>\n",
       "      <th>...</th>\n",
       "      <th>zuko</th>\n",
       "      <th>zukovsky</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurgs</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwicks</th>\n",
       "      <th>zwigoffs</th>\n",
       "      <th>zycie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 42210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0009f  000acre  000aweek  000foot  000paltry  007  007esque  \\\n",
       "0      1    0      0        0         0        0          0    0         0   \n",
       "1      0    0      0        0         0        0          0    0         0   \n",
       "2      0    0      0        0         0        0          0    0         0   \n",
       "3      0    1      0        0         0        0          0    0         0   \n",
       "4      0    0      0        0         0        0          0    0         0   \n",
       "...   ..  ...    ...      ...       ...      ...        ...  ...       ...   \n",
       "1995   0    0      0        0         0        0          0    0         0   \n",
       "1996   0    0      0        0         0        0          0    0         0   \n",
       "1997   0    0      0        0         0        0          0    0         0   \n",
       "1998   0    0      0        0         0        0          0    0         0   \n",
       "1999   0    0      0        0         0        0          0    0         0   \n",
       "\n",
       "      00s  ...  zuko  zukovsky  zulu  zundel  zurgs  zweibel  zwick  zwicks  \\\n",
       "0       0  ...     0         0     0       0      0        0      0       0   \n",
       "1       0  ...     0         0     0       0      0        0      0       0   \n",
       "2       0  ...     0         0     0       0      0        0      0       0   \n",
       "3       0  ...     0         0     0       0      0        0      0       0   \n",
       "4       0  ...     0         0     0       0      0        0      0       0   \n",
       "...   ...  ...   ...       ...   ...     ...    ...      ...    ...     ...   \n",
       "1995    0  ...     0         0     0       0      0        0      0       0   \n",
       "1996    0  ...     0         0     0       0      0        0      0       0   \n",
       "1997    0  ...     0         0     0       0      0        0      0       0   \n",
       "1998    0  ...     0         0     0       0      0        0      0       0   \n",
       "1999    0  ...     0         0     0       0      0        0      0       0   \n",
       "\n",
       "      zwigoffs  zycie  \n",
       "0            0      0  \n",
       "1            0      0  \n",
       "2            0      0  \n",
       "3            0      0  \n",
       "4            0      0  \n",
       "...        ...    ...  \n",
       "1995         0      0  \n",
       "1996         0      0  \n",
       "1997         0      0  \n",
       "1998         0      0  \n",
       "1999         0      0  \n",
       "\n",
       "[2000 rows x 42210 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=data[\"review\"]\n",
    "count_vector = CountVectorizer() \n",
    "X = count_vector.fit_transform(corpus).toarray()\n",
    "doc_array =  count_vector.fit_transform(corpus).toarray()\n",
    "frequency_matrix = pd.DataFrame(doc_array,columns=count_vector.get_feature_names_out())\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency and Inverse Document Frequency\n",
    "- Term Frequency measures how frequently a term occurs in a document.\n",
    "- Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "        TF(t) = (Number of times term t appears in a document)\n",
    "\n",
    "- Inverse Document Frequency measures how important a term is. While computing TF, all terms are considered equally important. \n",
    "- However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "        IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content  :  0.04125421116380646\n",
      "drug  :  0.03644017803575024\n",
      "language  :  0.0382730971684039\n",
      "sexuality  :  0.04961451434538843\n",
      "violencegore  :  0.07012369381991296\n",
      "00  :  0.06338699066308041\n",
      "however  :  0.020587726387926827\n",
      "bad  :  0.019089534661053946\n",
      "half  :  0.028227001615241932\n",
      "actually  :  0.02183892537054943\n",
      "irish  :  0.0515648344873698\n",
      "attempt  :  0.025890136853032694\n",
      "imagining  :  0.06618297509467609\n",
      "mouth  :  0.03913544645479638\n",
      "opened  :  0.04774485033631455\n",
      "cringed  :  0.07012369381991296\n",
      "surprise  :  0.030660214757371997\n",
      "big  :  0.021232826229687245\n",
      "role  :  0.01953256884116155\n",
      "supporting  :  0.030051309512897202\n",
      "great  :  0.01995490474071171\n",
      "log  :  0.06338699066308041\n",
      "dalmatian  :  0.06121825664997963\n",
      "102  :  0.06618297509467609\n",
      "richardson  :  0.052709568781011004\n",
      "secret  :  0.03345619918896767\n",
      "gould  :  0.0646847833678032\n",
      "joe  :  0.036906780032440974\n",
      "holm  :  0.057948080210970664\n",
      "ians  :  0.0768603969767455\n",
      "accent  :  0.07691645346442158\n",
      "british  :  0.03845822673221079\n",
      "handling  :  0.05231281948004631\n",
      "deftly  :  0.06338699066308041\n",
      "performance  :  0.018617162522241843\n",
      "strong  :  0.062441335210924015\n",
      "typically  :  0.05231281948004631\n",
      "turning  :  0.04125421116380646\n",
      "dreamy  :  0.06224225636943922\n",
      "solid  :  0.03451750800697392\n",
      "acting  :  0.02263095684163803\n",
      "creepy  :  0.04125421116380646\n",
      "one  :  0.010949727728362087\n",
      "surroundings  :  0.05665028750624787\n",
      "prague  :  0.0768603969767455\n",
      "original  :  0.02479969085473881\n",
      "design  :  0.04019290234580021\n",
      "production  :  0.02966558396060754\n",
      "love  :  0.0201802831468739\n",
      "shakespeare  :  0.05022215020982664\n",
      "child  :  0.026974157105243333\n",
      "martin  :  0.037565948404864866\n",
      "winner  :  0.042731657934593395\n",
      "oscar  :  0.0338552761173501\n",
      "blackandwhite  :  0.06224225636943922\n",
      "comparison  :  0.04019290234580021\n",
      "pale  :  0.0515648344873698\n",
      "violence  :  0.03139982490400139\n",
      "though  :  0.02084370267115874\n",
      "peak  :  0.04932437315065862\n",
      "twin  :  0.04704196523076726\n",
      "flashback  :  0.03944078072474191\n",
      "crazy  :  0.041635397597976895\n",
      "remind  :  0.050870324227869634\n",
      "scene  :  0.014976272180200933\n",
      "killing  :  0.036748743896161555\n",
      "flashy  :  0.04774485033631455\n",
      "helped  :  0.04482813133053726\n",
      "victorianera  :  0.0768603969767455\n",
      "dreariness  :  0.07291967825150864\n",
      "capture  :  0.04019290234580021\n",
      "ably  :  0.06029193622745785\n",
      "deming  :  0.0768603969767455\n",
      "cinematographer  :  0.042168079299889014\n",
      "manson  :  0.06121825664997963\n",
      "marilyn  :  0.057277537924742765\n",
      "comment  :  0.04113040046735314\n",
      "finalized  :  0.07291967825150864\n",
      "music  :  0.027812005545432564\n",
      "color  :  0.03765157898731275\n",
      "finished  :  0.04904264304103734\n",
      "completely  :  0.02655335509352115\n",
      "wasnt  :  0.060102619025794404\n",
      "saw  :  0.031628518002135377\n",
      "print  :  0.05121137705413811\n",
      "hollow  :  0.04850255919997359\n",
      "sleepy  :  0.056061076307491166\n",
      "time  :  0.026604214424167504\n",
      "ape  :  0.0515648344873698\n",
      "planet  :  0.035850966836993535\n",
      "burton  :  0.04774485033631455\n",
      "tim  :  0.03913544645479638\n",
      "much  :  0.01528134020391013\n",
      "surprising  :  0.03964979971041608\n",
      "bleak  :  0.04617748388157446\n",
      "dark  :  0.0314906574178196\n",
      "certainly  :  0.02832533980565724\n",
      "appearance  :  0.03502915288630849\n",
      "onto  :  0.03765157898731275\n",
      "see  :  0.032612592943190216\n",
      "sense  :  0.025197395293853384\n",
      "make  :  0.02767234742436733\n",
      "itll  :  0.05231281948004631\n",
      "worry  :  0.04750486263117288\n",
      "star  :  0.020889583204370323\n",
      "guttenberg  :  0.06795495980681218\n",
      "steve  :  0.037565948404864866\n",
      "made  :  0.019519246003933014\n",
      "carwho  :  0.0768603969767455\n",
      "electric  :  0.057277537924742765\n",
      "back  :  0.01958604357433789\n",
      "hold  :  0.03280802517092468\n",
      "day  :  0.020858972129501937\n",
      "simpson  :  0.055505553212606674\n",
      "song  :  0.035228183901103764\n",
      "stonecutter  :  0.0768603969767455\n",
      "whistling  :  0.0768603969767455\n",
      "ending  :  0.028001342862507644\n",
      "act  :  0.029043061836166645\n",
      "ghastly  :  0.06121825664997963\n",
      "committing  :  0.057948080210970664\n",
      "capable  :  0.043176881192582776\n",
      "could  :  0.01778060269065754\n",
      "englishman  :  0.0646847833678032\n",
      "indian  :  0.05022215020982664\n",
      "jew  :  0.051931633045875884\n",
      "blame  :  0.0405339551720687\n",
      "finger  :  0.04681852991379276\n",
      "point  :  0.02177154896042929\n",
      "blindly  :  0.06618297509467609\n",
      "local  :  0.03280802517092468\n",
      "watch  :  0.023393623118429956\n",
      "funny  :  0.023139133035711114\n",
      "viewer  :  0.0282596705812496\n",
      "hidden  :  0.042445784142011764\n",
      "keeping  :  0.04076815947434034\n",
      "job  :  0.024242894388318094\n",
      "good  :  0.030103292335478395\n",
      "rables  :  0.07291967825150864\n",
      "mi  :  0.0646847833678032\n",
      "le  :  0.026309350439268235\n",
      "yglesias  :  0.07291967825150864\n",
      "rafael  :  0.06618297509467609\n",
      "limit  :  0.05054083476791022\n",
      "vertical  :  0.06795495980681218\n",
      "hayes  :  0.0646847833678032\n",
      "terry  :  0.04850255919997359\n",
      "screenwriter  :  0.03280802517092468\n",
      "cloaking  :  0.0768603969767455\n",
      "bother  :  0.044649795900691985\n",
      "slay  :  0.07291967825150864\n",
      "chooses  :  0.04824337028693428\n",
      "reason  :  0.024028810661282915\n",
      "killer  :  0.03280802517092468\n",
      "identity  :  0.08327079519595379\n",
      "theory  :  0.04681852991379276\n",
      "interesting  :  0.023594022140580043\n",
      "unique  :  0.03855213092394039\n",
      "particular  :  0.035363172296588224\n",
      "go  :  0.016098091643679424\n",
      "wont  :  0.030535343554808584\n",
      "briefed  :  0.0768603969767455\n",
      "need  :  0.024776795653959426\n",
      "anyone  :  0.026444151604896188\n",
      "think  :  0.020528463958967066\n",
      "stomach  :  0.0515648344873698\n",
      "cant  :  0.02325576253411417\n",
      "surgeon  :  0.06121825664997963\n",
      "police  :  0.03291309655358354\n",
      "even  :  0.041911929912044565\n",
      "gruesome  :  0.05231281948004631\n",
      "horribly  :  0.04704196523076726\n",
      "investigate  :  0.04638650286724863\n",
      "proceeds  :  0.05121137705413811\n",
      "isnt  :  0.021185300422337593\n",
      "graham  :  0.10108166953582044\n",
      "heather  :  0.049913584349415334\n",
      "kelly  :  0.04597286562417846\n",
      "mary  :  0.040305262073934714\n",
      "named  :  0.028525052481404283\n",
      "befriends  :  0.051931633045875884\n",
      "arriving  :  0.055505553212606674\n",
      "upon  :  0.030051309512897202\n",
      "opium  :  0.07291967825150864\n",
      "absinthe  :  0.07291967825150864\n",
      "amount  :  0.03496370477207448\n",
      "copious  :  0.0646847833678032\n",
      "quell  :  0.07012369381991296\n",
      "try  :  0.02290999059717452\n",
      "unsuccessfully  :  0.06121825664997963\n",
      "dream  :  0.03234877399387314\n",
      "prophetic  :  0.06618297509467609\n",
      "widower  :  0.06338699066308041\n",
      "case  :  0.027089517607827535\n",
      "crack  :  0.04577246660202836\n",
      "blow  :  0.04065035187149127\n",
      "depp  :  0.1080147229714676\n",
      "johnny  :  0.04638650286724863\n",
      "abberline  :  0.153720793953491\n",
      "frederick  :  0.0646847833678032\n",
      "inspector  :  0.050870324227869634\n",
      "call  :  0.029478314038949533\n",
      "enough  :  0.040822024558004405\n",
      "coltrane  :  0.07012369381991296\n",
      "robbie  :  0.05665028750624787\n",
      "godley  :  0.0768603969767455\n",
      "peter  :  0.06499891862102673\n",
      "copper  :  0.06618297509467609\n",
      "turn  :  0.04401887732109376\n",
      "stiff  :  0.05231281948004631\n",
      "first  :  0.03253621556998158\n",
      "precision  :  0.05944627193784355\n",
      "surgical  :  0.07291967825150864\n",
      "profession  :  0.04904264304103734\n",
      "carving  :  0.0646847833678032\n",
      "psychopath  :  0.056061076307491166\n",
      "mysterious  :  0.03791309274385944\n",
      "nervous  :  0.04904264304103734\n",
      "unfortunate  :  0.0869708941059882\n",
      "whore  :  0.05866833620270843\n",
      "place  :  0.04522527273656855\n",
      "sooty  :  0.0768603969767455\n",
      "filthy  :  0.06795495980681218\n",
      "end  :  0.03565047312246589\n",
      "east  :  0.05231281948004631\n",
      "london  :  0.09235496776314892\n",
      "1888  :  0.07291967825150864\n",
      "whitechapel  :  0.153720793953491\n",
      "course  :  0.02325576253411417\n",
      "question  :  0.026691607897544253\n",
      "society  :  0.03739691791420454\n",
      "ii  :  0.03975601943693358\n",
      "menace  :  0.04500980032523644\n",
      "behind  :  0.028129648464992416\n",
      "genius  :  0.04150666713010174\n",
      "mad  :  0.043176881192582776\n",
      "crime  :  0.06941246508670468\n",
      "street  :  0.03427144402264946\n",
      "violent  :  0.037481065697315304\n",
      "feature  :  0.027534740865816917\n",
      "ghetto  :  0.11212215261498233\n",
      "set  :  0.021555746572643406\n",
      "thats  :  0.022235674671514126\n",
      "better  :  0.020194544350072955\n",
      "riddle  :  0.06795495980681218\n",
      "anything  :  0.02317785423528068\n",
      "well  :  0.016363862335266925\n",
      "top  :  0.029703474878917696\n",
      "carrot  :  0.06338699066308041\n",
      "casting  :  0.03782513731790417\n",
      "ludicrous  :  0.04727065832890125\n",
      "almost  :  0.021638181403030584\n",
      "seems  :  0.04021857869036463\n",
      "direct  :  0.0860523917518851\n",
      "brother  :  0.029703474878917696\n",
      "getting  :  0.027060548858403616\n",
      "hughes  :  0.10174064845573927\n",
      "allen  :  0.04203214689894159\n",
      "albert  :  0.05022215020982664\n",
      "director  :  0.017836427130386714\n",
      "block  :  0.04538365442542175\n",
      "stumbling  :  0.05665028750624787\n",
      "another  :  0.018751556983754733\n",
      "find  :  0.019153433696200147\n",
      "might  :  0.02345331015291792\n",
      "thing  :  0.0168775880385106\n",
      "past  :  0.028728955237770304\n",
      "get  :  0.027568756684096453\n",
      "source  :  0.042168079299889014\n",
      "dismiss  :  0.057277537924742765\n",
      "dont  :  0.0939426743282937\n",
      "word  :  0.053438992064270247\n",
      "footnote  :  0.07012369381991296\n",
      "nothing  :  0.021588636648824697\n",
      "consist  :  0.06121825664997963\n",
      "30  :  0.042877811188555895\n",
      "nearly  :  0.029515480463730797\n",
      "includes  :  0.03874270581886534\n",
      "long  :  0.021605123520015423\n",
      "page  :  0.04799091432063901\n",
      "500  :  0.057277537924742765\n",
      "novel  :  0.03291309655358354\n",
      "graphic  :  0.040418935980725296\n",
      "odd  :  0.03903576344519582\n",
      "little  :  0.03460540086001447\n",
      "look  :  0.03673127990819446\n",
      "starting  :  0.08860530312339483\n",
      "jackson  :  0.04364348318927352\n",
      "michael  :  0.028937227669829157\n",
      "saying  :  0.03427144402264946\n",
      "would  :  0.015411333335327054\n",
      "ripper  :  0.21875903475452588\n",
      "jack  :  0.07100012399901008\n",
      "subject  :  0.03397237401862256\n",
      "researched  :  0.0768603969767455\n",
      "thoroughly  :  0.042587669993826086\n",
      "say  :  0.07839783342656784\n",
      "watchman  :  0.0768603969767455\n",
      "called  :  0.056782911525833125\n",
      "series  :  0.02762628636120287\n",
      "12part  :  0.0768603969767455\n",
      "80  :  0.040418935980725296\n",
      "mid  :  0.05944627193784355\n",
      "level  :  0.03041205638488378\n",
      "new  :  0.019115043883667467\n",
      "whole  :  0.05112471426306147\n",
      "medium  :  0.0405339551720687\n",
      "brought  :  0.03550006199950504\n",
      "campbell  :  0.13791859687253538\n",
      "eddie  :  0.0423059398842048\n",
      "moore  :  0.13342402169191672\n",
      "alan  :  0.04189808948753143\n",
      "created  :  0.034769963973269195\n",
      "starter  :  0.06224225636943922\n",
      "hell  :  0.15745328708909798\n",
      "like  :  0.05031336260817901\n",
      "really  :  0.03565047312246589\n",
      "never  :  0.03608018499425377\n",
      "there  :  0.02110660356939569\n",
      "world  :  0.04298059691681878\n",
      "ghost  :  0.04557611632321376\n",
      "crowd  :  0.04176585604314104\n",
      "arthouse  :  0.06338699066308041\n",
      "casper  :  0.055505553212606674\n",
      "kid  :  0.027565160321749434\n",
      "toward  :  0.04008182675696022\n",
      "geared  :  0.057948080210970664\n",
      "spawn  :  0.054980073443766826\n",
      "superman  :  0.057277537924742765\n",
      "batman  :  0.04176585604314104\n",
      "superheroes  :  0.06224225636943922\n",
      "theyre  :  0.028424683173866337\n",
      "whether  :  0.032398743297963835\n",
      "success  :  0.03220038930648647\n",
      "plenty  :  0.03782513731790417\n",
      "book  :  0.12100027665802043\n",
      "comic  :  0.14683829999235287\n",
      "adapted  :  0.04660011604267335\n",
      "film  :  0.06471441643287094\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Get the TF-IDF score for the first document\n",
    "doc1_tfidf = tfidf_matrix[0]\n",
    "\n",
    "# Print the feature name and corresponding TF-IDF score for the first document\n",
    "# for col in doc1_tfidf.nonzero()[1]:\n",
    "#     print(feature_names[col], ' : ', doc1_tfidf[0, col])\n",
    "    \n",
    "frequency_matrix.loc[\"idf\",:]=doc1_tfidf.toarray()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0009f</th>\n",
       "      <th>000acre</th>\n",
       "      <th>000aweek</th>\n",
       "      <th>000foot</th>\n",
       "      <th>000paltry</th>\n",
       "      <th>007</th>\n",
       "      <th>007esque</th>\n",
       "      <th>00s</th>\n",
       "      <th>...</th>\n",
       "      <th>zuko</th>\n",
       "      <th>zukovsky</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurgs</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwicks</th>\n",
       "      <th>zwigoffs</th>\n",
       "      <th>zycie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idf</th>\n",
       "      <td>0.063387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows × 42210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00  000  0009f  000acre  000aweek  000foot  000paltry  007  \\\n",
       "0     1.000000  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "1     0.000000  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "2     0.000000  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "3     0.000000  1.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "4     0.000000  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "...        ...  ...    ...      ...       ...      ...        ...  ...   \n",
       "1996  0.000000  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "1997  0.000000  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "1998  0.000000  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "1999  0.000000  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "idf   0.063387  0.0    0.0      0.0       0.0      0.0        0.0  0.0   \n",
       "\n",
       "      007esque  00s  ...  zuko  zukovsky  zulu  zundel  zurgs  zweibel  zwick  \\\n",
       "0          0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "1          0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "2          0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "3          0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "4          0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "...        ...  ...  ...   ...       ...   ...     ...    ...      ...    ...   \n",
       "1996       0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "1997       0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "1998       0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "1999       0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "idf        0.0  0.0  ...   0.0       0.0   0.0     0.0    0.0      0.0    0.0   \n",
       "\n",
       "      zwicks  zwigoffs  zycie  \n",
       "0        0.0       0.0    0.0  \n",
       "1        0.0       0.0    0.0  \n",
       "2        0.0       0.0    0.0  \n",
       "3        0.0       0.0    0.0  \n",
       "4        0.0       0.0    0.0  \n",
       "...      ...       ...    ...  \n",
       "1996     0.0       0.0    0.0  \n",
       "1997     0.0       0.0    0.0  \n",
       "1998     0.0       0.0    0.0  \n",
       "1999     0.0       0.0    0.0  \n",
       "idf      0.0       0.0    0.0  \n",
       "\n",
       "[2001 rows x 42210 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create WordCloud for movies reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.8.2.2.tar.gz (220 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from wordcloud) (1.23.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from wordcloud) (9.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from wordcloud) (3.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tnouali\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Building wheels for collected packages: wordcloud\n",
      "  Building wheel for wordcloud (setup.py): started\n",
      "  Building wheel for wordcloud (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for wordcloud\n",
      "Failed to build wordcloud\n",
      "Installing collected packages: wordcloud\n",
      "  Running setup.py install for wordcloud: started\n",
      "  Running setup.py install for wordcloud: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [26 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\tokenization.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\_version.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\__init__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\__main__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\stopwords -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      UPDATING build\\lib.win-amd64-cpython-311\\wordcloud/_version.py\n",
      "      set build\\lib.win-amd64-cpython-311\\wordcloud/_version.py to '1.8.2.2'\n",
      "      running build_ext\n",
      "      building 'wordcloud.query_integral_image' extension\n",
      "      creating build\\temp.win-amd64-cpython-311\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\\wordcloud\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.35.32215\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD \"-IC:\\Program Files\\Python311\\include\" \"-IC:\\Program Files\\Python311\\Include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.35.32215\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.35.32215\\ATLMFC\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22000.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22000.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22000.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22000.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22000.0\\\\cppwinrt\" /Tcwordcloud/query_integral_image.c /Fobuild\\temp.win-amd64-cpython-311\\Release\\wordcloud/query_integral_image.obj\n",
      "      query_integral_image.c\n",
      "      wordcloud/query_integral_image.c(196): fatal error C1083: Impossible d'ouvrir le fichier includeÿ: 'longintrepr.h'ÿ: No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.35.32215\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for wordcloud\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for wordcloud did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [28 lines of output]\n",
      "      running install\n",
      "      C:\\Users\\tNouali\\AppData\\Roaming\\Python\\Python311\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "        warnings.warn(\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\color_from_image.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\tokenization.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\wordcloud.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\wordcloud_cli.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\_version.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\__init__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\__main__.py -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\stopwords -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      copying wordcloud\\DroidSansMono.ttf -> build\\lib.win-amd64-cpython-311\\wordcloud\n",
      "      UPDATING build\\lib.win-amd64-cpython-311\\wordcloud/_version.py\n",
      "      set build\\lib.win-amd64-cpython-311\\wordcloud/_version.py to '1.8.2.2'\n",
      "      running build_ext\n",
      "      building 'wordcloud.query_integral_image' extension\n",
      "      creating build\\temp.win-amd64-cpython-311\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\\wordcloud\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.35.32215\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD \"-IC:\\Program Files\\Python311\\include\" \"-IC:\\Program Files\\Python311\\Include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.35.32215\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.35.32215\\ATLMFC\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22000.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22000.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22000.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22000.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22000.0\\\\cppwinrt\" /Tcwordcloud/query_integral_image.c /Fobuild\\temp.win-amd64-cpython-311\\Release\\wordcloud/query_integral_image.obj\n",
      "      query_integral_image.c\n",
      "      wordcloud/query_integral_image.c(196): fatal error C1083: Impossible d'ouvrir le fichier includeÿ: 'longintrepr.h'ÿ: No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.35.32215\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> wordcloud\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pylab as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=800, height=600).generate(\" \".join(data['text']))\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "532px",
    "width": "198px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
